{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "잡케어추천알고리즘_TabNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMzDjo1gA+P33avX2vxZwz2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doo52oh/Dacon/blob/main/%EC%9E%A1%EC%BC%80%EC%96%B4%EC%B6%94%EC%B2%9C/%EC%9E%A1%EC%BC%80%EC%96%B4%EC%B6%94%EC%B2%9C%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98_TabNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwTv2Q1feVa3",
        "outputId": "fc2a83e1-4c8e-47ee-de19-1cd4518dac5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-tabnet==3.1.1\n",
            "  Downloading pytorch_tabnet-3.1.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: torch<2.0,>=1.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet==3.1.1) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet==3.1.1) (1.19.5)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet==3.1.1) (1.0.2)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet==3.1.1) (1.4.1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.36 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet==3.1.1) (4.62.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet==3.1.1) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet==3.1.1) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0,>=1.2->pytorch-tabnet==3.1.1) (3.10.0.2)\n",
            "Installing collected packages: pytorch-tabnet\n",
            "Successfully installed pytorch-tabnet-3.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-tabnet==3.1.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from typing import Dict\n",
        "from datetime import datetime\n",
        "## preprocessing\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "## modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from pytorch_tabnet.tab_model  import TabNetClassifier \n",
        "from pytorch_tabnet.metrics import Metric\n",
        "\n",
        "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wia8aS7PfBms",
        "outputId": "9d34298d-0a90-4a56-bb4d-f61698c0d52e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/drive/MyDrive/Dacon/JobCare/data/\"\n",
        "SUBMIT_PATH = \"/content/drive/MyDrive/Dacon/JobCare/submit/\"\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "qTB8Tci7fCV2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(f'{DATA_PATH}train.csv')\n",
        "test_data = pd.read_csv(f'{DATA_PATH}test.csv')\n",
        "\n",
        "d_code = pd.read_csv(f'{DATA_PATH}속성_D_코드.csv', index_col=0).T.to_dict()\n",
        "h_code = pd.read_csv(f'{DATA_PATH}속성_H_코드.csv', index_col=0).T.to_dict()\n",
        "l_code = pd.read_csv(f'{DATA_PATH}속성_L_코드.csv', index_col=0).T.to_dict()\n",
        "\n",
        "print(\"train_data.shape: \", train_data.shape)\n",
        "print(\"test_data.shape: \", test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rbl7Gv87fIiq",
        "outputId": "5c8e1a82-d9cd-47a5-b2c2-b633b4e3c2cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data.shape:  (501951, 35)\n",
            "test_data.shape:  (46404, 34)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_code(\n",
        "    df: pd.DataFrame,\n",
        "    d_code: Dict[int, Dict[str, int]], \n",
        "    h_code: Dict[int, Dict[str, int]], \n",
        "    l_code: Dict[int, Dict[str, int]],\n",
        ") -> pd.DataFrame:\n",
        "    \n",
        "    # Copy input data\n",
        "    df = df.copy()   \n",
        "\n",
        "    # D Code\n",
        "    df['person_prefer_d_1_n'] = df['person_prefer_d_1'].apply(lambda x: d_code[x]['속성 D 세분류코드'])\n",
        "    df['person_prefer_d_1_s'] = df['person_prefer_d_1'].apply(lambda x: d_code[x]['속성 D 소분류코드'])\n",
        "    df['person_prefer_d_1_m'] = df['person_prefer_d_1'].apply(lambda x: d_code[x]['속성 D 중분류코드'])\n",
        "    df['person_prefer_d_1_l'] = df['person_prefer_d_1'].apply(lambda x: d_code[x]['속성 D 대분류코드'])\n",
        "\n",
        "    df['person_prefer_d_2_n'] = df['person_prefer_d_2'].apply(lambda x: d_code[x]['속성 D 세분류코드'])\n",
        "    df['person_prefer_d_2_s'] = df['person_prefer_d_2'].apply(lambda x: d_code[x]['속성 D 소분류코드'])\n",
        "    df['person_prefer_d_2_m'] = df['person_prefer_d_2'].apply(lambda x: d_code[x]['속성 D 중분류코드'])\n",
        "    df['person_prefer_d_2_l'] = df['person_prefer_d_2'].apply(lambda x: d_code[x]['속성 D 대분류코드'])\n",
        "\n",
        "    df['person_prefer_d_3_n'] = df['person_prefer_d_3'].apply(lambda x: d_code[x]['속성 D 세분류코드'])\n",
        "    df['person_prefer_d_3_s'] = df['person_prefer_d_3'].apply(lambda x: d_code[x]['속성 D 소분류코드'])\n",
        "    df['person_prefer_d_3_m'] = df['person_prefer_d_3'].apply(lambda x: d_code[x]['속성 D 중분류코드'])\n",
        "    df['person_prefer_d_3_l'] = df['person_prefer_d_3'].apply(lambda x: d_code[x]['속성 D 대분류코드'])\n",
        "\n",
        "    df['contents_attribute_d_n'] = df['contents_attribute_d'].apply(lambda x: d_code[x]['속성 D 세분류코드'])\n",
        "    df['contents_attribute_d_s'] = df['contents_attribute_d'].apply(lambda x: d_code[x]['속성 D 소분류코드'])\n",
        "    df['contents_attribute_d_m'] = df['contents_attribute_d'].apply(lambda x: d_code[x]['속성 D 중분류코드'])\n",
        "    df['contents_attribute_d_l'] = df['contents_attribute_d'].apply(lambda x: d_code[x]['속성 D 대분류코드'])\n",
        "\n",
        "    # H Code\n",
        "    df['person_prefer_h_1_l'] = df['person_prefer_h_1'].apply(lambda x: h_code[x]['속성 H 대분류코드'])\n",
        "    df['person_prefer_h_1_m'] = df['person_prefer_h_1'].apply(lambda x: h_code[x]['속성 H 중분류코드'])\n",
        "    \n",
        "    df['person_prefer_h_2_l'] = df['person_prefer_h_2'].apply(lambda x: h_code[x]['속성 H 대분류코드'])\n",
        "    df['person_prefer_h_2_m'] = df['person_prefer_h_2'].apply(lambda x: h_code[x]['속성 H 중분류코드'])\n",
        "    \n",
        "    df['person_prefer_h_3_l'] = df['person_prefer_h_3'].apply(lambda x: h_code[x]['속성 H 대분류코드'])\n",
        "    df['person_prefer_h_3_m'] = df['person_prefer_h_3'].apply(lambda x: h_code[x]['속성 H 중분류코드'])\n",
        "\n",
        "    df['contents_attribute_h_l'] = df['contents_attribute_h'].apply(lambda x: h_code[x]['속성 H 대분류코드'])\n",
        "    df['contents_attribute_h_m'] = df['contents_attribute_h'].apply(lambda x: h_code[x]['속성 H 중분류코드'])\n",
        "\n",
        "    # L Code\n",
        "    df['contents_attribute_l_n'] = df['contents_attribute_l'].apply(lambda x: l_code[x]['속성 L 세분류코드'])\n",
        "    df['contents_attribute_l_s'] = df['contents_attribute_l'].apply(lambda x: l_code[x]['속성 L 소분류코드'])\n",
        "    df['contents_attribute_l_m'] = df['contents_attribute_l'].apply(lambda x: l_code[x]['속성 L 중분류코드'])\n",
        "    df['contents_attribute_l_l'] = df['contents_attribute_l'].apply(lambda x: l_code[x]['속성 L 대분류코드'])\n",
        "    \n",
        "    return df\n",
        "\n",
        "train_data = add_code(train_data, d_code, h_code, l_code)\n",
        "test_data = add_code(test_data, d_code, h_code, l_code)\n",
        "print(\"train_data.shape: \", train_data.shape)\n",
        "print(\"test_data.shape: \", test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4XNO9D-fJzr",
        "outputId": "be33c465-a5df-4445-fab5-92c0a6e8ea9e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data.shape:  (501951, 63)\n",
            "test_data.shape:  (46404, 62)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_contents_open_dt(data):\n",
        "    data['contents_open_dt'] = data['contents_open_dt'].astype('str')\n",
        "    DATE = data['contents_open_dt'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
        "    \n",
        "    DATE = pd.DataFrame(DATE)\n",
        "    DATE = DATE.rename(columns = {'contents_open_dt': 'date'})\n",
        "    \n",
        "    DATE['Y'] = DATE['date'].apply(lambda x: x.timetuple()[0])\n",
        "    DATE['M'] = DATE['date'].apply(lambda x: x.timetuple()[1])\n",
        "    DATE['D'] = DATE['date'].apply(lambda x: x.timetuple()[2])\n",
        "    DATE['id'] = data['id']\n",
        "    \n",
        "    data = data.merge(DATE, on = 'id', how = 'left')\n",
        "    data = data.drop(columns = ['date', 'contents_open_dt'])\n",
        "    return data\n",
        "\n",
        "train_data = preprocessing_contents_open_dt(train_data)\n",
        "test_data = preprocessing_contents_open_dt(test_data)\n",
        "\n",
        "# 안전하게 확인하고 넘어 갑시다. \n",
        "train_data_labels = train_data['target']\n",
        "train_data, test_data = train_data.align(test_data, join = 'inner', axis = 1)\n",
        "train_data['target'] = train_data_labels\n",
        "print(\"train_data.shape: \", train_data.shape)\n",
        "print(\"test_data.shape: \", test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWYiFWQtfL46",
        "outputId": "2c222465-4187-4642-89dd-c5a57bed7be7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data.shape:  (501951, 65)\n",
            "test_data.shape:  (46404, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job_data = train_data.copy()\n",
        "job_data_test = test_data.copy()"
      ],
      "metadata": {
        "id": "si9s72C0fNN_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop_cols = ['id', 'person_rn', 'contents_rn', 'Y', 'M', 'D','person_prefer_f','person_prefer_g']\n",
        "\n",
        "job_df = job_data.drop(drop_cols, axis=1)\n",
        "job_test_df = job_data_test.drop(drop_cols, axis=1)"
      ],
      "metadata": {
        "id": "2A0hIdCtfPK3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M7a7qZtfVoK",
        "outputId": "c326ab95-d0e5-44d2-fc70-b4490b5231fa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 501951 entries, 0 to 501950\n",
            "Data columns (total 57 columns):\n",
            " #   Column                  Non-Null Count   Dtype\n",
            "---  ------                  --------------   -----\n",
            " 0   d_l_match_yn            501951 non-null  bool \n",
            " 1   d_m_match_yn            501951 non-null  bool \n",
            " 2   d_s_match_yn            501951 non-null  bool \n",
            " 3   h_l_match_yn            501951 non-null  bool \n",
            " 4   h_m_match_yn            501951 non-null  bool \n",
            " 5   h_s_match_yn            501951 non-null  bool \n",
            " 6   person_attribute_a      501951 non-null  int64\n",
            " 7   person_attribute_a_1    501951 non-null  int64\n",
            " 8   person_attribute_b      501951 non-null  int64\n",
            " 9   person_prefer_c         501951 non-null  int64\n",
            " 10  person_prefer_d_1       501951 non-null  int64\n",
            " 11  person_prefer_d_2       501951 non-null  int64\n",
            " 12  person_prefer_d_3       501951 non-null  int64\n",
            " 13  person_prefer_e         501951 non-null  int64\n",
            " 14  person_prefer_h_1       501951 non-null  int64\n",
            " 15  person_prefer_h_2       501951 non-null  int64\n",
            " 16  person_prefer_h_3       501951 non-null  int64\n",
            " 17  contents_attribute_i    501951 non-null  int64\n",
            " 18  contents_attribute_a    501951 non-null  int64\n",
            " 19  contents_attribute_j_1  501951 non-null  int64\n",
            " 20  contents_attribute_j    501951 non-null  int64\n",
            " 21  contents_attribute_c    501951 non-null  int64\n",
            " 22  contents_attribute_k    501951 non-null  int64\n",
            " 23  contents_attribute_l    501951 non-null  int64\n",
            " 24  contents_attribute_d    501951 non-null  int64\n",
            " 25  contents_attribute_m    501951 non-null  int64\n",
            " 26  contents_attribute_e    501951 non-null  int64\n",
            " 27  contents_attribute_h    501951 non-null  int64\n",
            " 28  person_prefer_d_1_n     501951 non-null  int64\n",
            " 29  person_prefer_d_1_s     501951 non-null  int64\n",
            " 30  person_prefer_d_1_m     501951 non-null  int64\n",
            " 31  person_prefer_d_1_l     501951 non-null  int64\n",
            " 32  person_prefer_d_2_n     501951 non-null  int64\n",
            " 33  person_prefer_d_2_s     501951 non-null  int64\n",
            " 34  person_prefer_d_2_m     501951 non-null  int64\n",
            " 35  person_prefer_d_2_l     501951 non-null  int64\n",
            " 36  person_prefer_d_3_n     501951 non-null  int64\n",
            " 37  person_prefer_d_3_s     501951 non-null  int64\n",
            " 38  person_prefer_d_3_m     501951 non-null  int64\n",
            " 39  person_prefer_d_3_l     501951 non-null  int64\n",
            " 40  contents_attribute_d_n  501951 non-null  int64\n",
            " 41  contents_attribute_d_s  501951 non-null  int64\n",
            " 42  contents_attribute_d_m  501951 non-null  int64\n",
            " 43  contents_attribute_d_l  501951 non-null  int64\n",
            " 44  person_prefer_h_1_l     501951 non-null  int64\n",
            " 45  person_prefer_h_1_m     501951 non-null  int64\n",
            " 46  person_prefer_h_2_l     501951 non-null  int64\n",
            " 47  person_prefer_h_2_m     501951 non-null  int64\n",
            " 48  person_prefer_h_3_l     501951 non-null  int64\n",
            " 49  person_prefer_h_3_m     501951 non-null  int64\n",
            " 50  contents_attribute_h_l  501951 non-null  int64\n",
            " 51  contents_attribute_h_m  501951 non-null  int64\n",
            " 52  contents_attribute_l_n  501951 non-null  int64\n",
            " 53  contents_attribute_l_s  501951 non-null  int64\n",
            " 54  contents_attribute_l_m  501951 non-null  int64\n",
            " 55  contents_attribute_l_l  501951 non-null  int64\n",
            " 56  target                  501951 non-null  int64\n",
            "dtypes: bool(6), int64(51)\n",
            "memory usage: 202.0 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = job_df.iloc[:,:-1]\n",
        "Y = job_df.iloc[:,-1]\n",
        "test = job_test_df"
      ],
      "metadata": {
        "id": "GJkvRvS5fXSA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size = 0.33, shuffle = True, stratify = Y, random_state = 1234)"
      ],
      "metadata": {
        "id": "tHizaQqEfcOg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXAogHT5fdt0",
        "outputId": "552b6b1c-2a1b-4352-db12-47b5f09d952e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((336307, 56), (165644, 56), (336307,), (165644,))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_idxs = []\n",
        "cat_dims = []\n",
        "for idx, col in enumerate(X_train.columns):\n",
        "    if 'match' not in col and col!='target': \n",
        "        le = LabelEncoder()\n",
        "        le.fit(X_train[col].values)\n",
        "        le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "        X_train[col] = X_train[col].apply(lambda x: le_dict.get(x, len(le_dict)))\n",
        "        X_valid[col] = X_valid[col].apply(lambda x: le_dict.get(x, len(le_dict)))\n",
        "        test[col] = test[col].apply(lambda x: le_dict.get(x, len(le_dict)))\n",
        "        cat_idxs.append(idx)\n",
        "        cat_dims.append(len(le_dict)+1)"
      ],
      "metadata": {
        "id": "B6Y_TdiMydSJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4-elMjF3lA0",
        "outputId": "74eedf78-4b5d-462e-d0a4-b9ef20250e8a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((336307, 56), (165644, 56), (336307,), (165644,))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TabNet"
      ],
      "metadata": {
        "id": "ZKQUDR5XfhOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = TabNetClassifier(cat_idxs=cat_idxs,\n",
        "                       cat_dims=cat_dims,\n",
        "                       cat_emb_dim=3,\n",
        "                       optimizer_fn=torch.optim.AdamW, # Any optimizer works here\n",
        "                       mask_type='entmax', # \"sparsemax\",\n",
        "                      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fOZw3fnffKT",
        "outputId": "a79d94c9-e2ca-4697-c351-a3aef15e4bff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device used : cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(\n",
        "    X_train=X_train, y_train=Y_train,\n",
        "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n",
        "    eval_name=['train', 'val'],\n",
        "    eval_metric=['logloss','auc'],\n",
        "    max_epochs=100 , patience=2,\n",
        "    batch_size=1024,\n",
        "    virtual_batch_size=256,\n",
        "    num_workers=1,\n",
        "    drop_last=False,\n",
        ") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "id": "Yw_A2Vzv1dYB",
        "outputId": "1cedf283-bce9-4d4b-faca-f2125bdcbf4f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ca4e633f415a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvirtual_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m ) \n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;31m# Apply predict epoch to all eval sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 2898, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1675, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1683, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 187336\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/utils.py\", line 30, in __getitem__\n    x, y = self.x[index], self.y[index]\n  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\", line 2906, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 2900, in get_loc\n    raise KeyError(key) from err\nKeyError: 187336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WE3D7seV2ceX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}